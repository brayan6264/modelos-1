{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**PROYECTO KAGGLE - Pruebas Saber Pro Colombia**\n",
        "## **Solución Alternativa**\n",
        "\n",
        "Modelo Predictivo a partir de la tabla de datos train.csv sobre el rendimiento de los estudiantes en las pruebas Saber Pro.\n",
        "\n",
        "Hecho por:\n",
        "- Juan Manuel Areiza Ospina - C.C. 1018226898\n",
        "- Samuel Puerta Patiño - C.C. 1023624795\n",
        "- Brayan Stiven Gómez Villa - C.C 1018224235"
      ],
      "metadata": {
        "id": "gCIJO_awQ40B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La solución utiliza XGBoost, un algoritmo robusto y eficiente para datos tabulares. El proceso incluye:\n",
        "\n",
        "1. **Carga de Datos:** Importación de los conjuntos de entrenamiento y prueba.\n",
        "\n",
        "2. **Ingeniería de Características:** Transformación profunda de variables categóricas (estrato, educación) y creación de nuevos índices sintéticos para capturar mejor el contexto socioeconómico del estudiante.\n",
        "\n",
        "3. **Codificación:** Uso de OrdinalEncoder y LabelEncoder para preparar los datos para el modelo.\n",
        "\n",
        "4. **Modelado:** Entrenamiento de un clasificador XGBoost con hiperparámetros optimizados para evitar el sobreajuste (overfitting)."
      ],
      "metadata": {
        "id": "Bb4wtbutg0lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Carga de Datos**"
      ],
      "metadata": {
        "id": "wV9b5aM8jn2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeYXc5SDc81R",
        "outputId": "59fda814-4224-41a6-d4db-24cb4c0d91f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip to /content\n",
            "\r  0% 0.00/29.9M [00:00<?, ?B/s]\n",
            "\r100% 29.9M/29.9M [00:00<00:00, 362MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV6uDA6Fc-7B",
        "outputId": "09f1ad39-fbf6-4ba9-dac6-a5e52087699e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip\n",
            "  inflating: submission_example.csv  \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Modelo**"
      ],
      "metadata": {
        "id": "5t_DW0ZujuUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzQRrP3ZecwD",
        "outputId": "acd0da0e-4e60-4766-bd62-fa127431b0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Cargando datos\n",
            "Procesando Features...\n",
            "Entrenando XGBoost...\n",
            "[0]\tvalidation_0-mlogloss:1.37977\n",
            "[50]\tvalidation_0-mlogloss:1.24610\n",
            "[100]\tvalidation_0-mlogloss:1.22130\n",
            "[150]\tvalidation_0-mlogloss:1.21144\n",
            "[200]\tvalidation_0-mlogloss:1.20690\n",
            "[250]\tvalidation_0-mlogloss:1.20432\n",
            "[300]\tvalidation_0-mlogloss:1.20237\n",
            "[350]\tvalidation_0-mlogloss:1.20117\n",
            "[400]\tvalidation_0-mlogloss:1.20035\n",
            "[450]\tvalidation_0-mlogloss:1.19990\n",
            "[499]\tvalidation_0-mlogloss:1.19956\n",
            "\n",
            "Evaluando modelo...\n",
            "Accuracy: 0.43477256317689533\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        alto       0.55      0.63      0.59     26343\n",
            "        bajo       0.47      0.56      0.51     25948\n",
            "  medio-alto       0.33      0.27      0.30     25743\n",
            "  medio-bajo       0.33      0.27      0.30     25841\n",
            "\n",
            "    accuracy                           0.43    103875\n",
            "   macro avg       0.42      0.43      0.42    103875\n",
            "weighted avg       0.42      0.43      0.42    103875\n",
            "\n",
            "Generando predicciones finales...\n",
            "ARCHIVO 'submission_xgb.csv' CREADO CON ÉXITO.\n"
          ]
        }
      ],
      "source": [
        "# INSTALACIÓN DE XGBOOST\n",
        "!pip install xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "#CARGA DE DATOS\n",
        "print(\"Cargando datos\")\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "submission_example = pd.read_csv('submission_example.csv')\n",
        "\n",
        "# PROCESAMIENTO\n",
        "def procesar_datos(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Limpieza básica\n",
        "    # Eliminar duplicados  o columnas vacías\n",
        "    cols_drop = ['F_TIENEINTERNET.1'] # Duplicada\n",
        "    df = df.drop(columns=[c for c in cols_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "    # Mapeos de Ordinales\n",
        "    # Mapa de Estrato\n",
        "    mapa_estrato = {\n",
        "        'Estrato 1': 1, 'Estrato 2': 2, 'Estrato 3': 3,\n",
        "        'Estrato 4': 4, 'Estrato 5': 5, 'Estrato 6': 6, 'Sin Estrato': 0\n",
        "    }\n",
        "    df['F_ESTRATOVIVIENDA'] = df['F_ESTRATOVIVIENDA'].map(mapa_estrato).fillna(0)\n",
        "\n",
        "    # Mapa de Educación (Aproximado por nivel)\n",
        "    mapa_edu = {\n",
        "        'Ninguno': 0, 'No sabe': 0, 'Primaria incompleta': 1, 'Primaria completa': 2,\n",
        "        'Secundaria (Bachillerato) incompleta': 3, 'Secundaria (Bachillerato) completa': 4,\n",
        "        'Técnica o tecnológica incompleta': 5, 'Técnica o tecnológica completa': 6,\n",
        "        'Educación profesional incompleta': 7, 'Educación profesional completa': 8,\n",
        "        'Postgrado': 9\n",
        "    }\n",
        "    # Aplicar a padre y madre\n",
        "    df['F_EDUCACIONPADRE'] = df['F_EDUCACIONPADRE'].map(mapa_edu).fillna(-1)\n",
        "    df['F_EDUCACIONMADRE'] = df['F_EDUCACIONMADRE'].map(mapa_edu).fillna(-1)\n",
        "\n",
        "    # Mapa Si/No\n",
        "    mapa_si_no = {'Si': 1, 'No': 0}\n",
        "    cols_binarias = ['F_TIENEINTERNET', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR', 'E_PAGOMATRICULAPROPIO']\n",
        "    for col in cols_binarias:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map(mapa_si_no).fillna(0)\n",
        "\n",
        "    # Creación de nuevas variables\n",
        "\n",
        "    # Índice de equipamiento tecnológico/hogar (suma de binarias)\n",
        "    df['SCORE_HOGAR'] = df['F_TIENEINTERNET'] + df['F_TIENELAVADORA'] + df['F_TIENEAUTOMOVIL'] + df['F_TIENECOMPUTADOR']\n",
        "\n",
        "    # Promedio de educación de los padres\n",
        "    df['EDU_PADRES_AVG'] = (df['F_EDUCACIONPADRE'] + df['F_EDUCACIONMADRE']) / 2\n",
        "\n",
        "    # Diferencia entre indicadores (si existen)\n",
        "    if 'INDICADOR_1' in df.columns and 'INDICADOR_2' in df.columns:\n",
        "        df['DELTA_IND'] = df['INDICADOR_1'] - df['INDICADOR_2']\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Procesando Features...\")\n",
        "X = procesar_datos(df_train.drop(columns=['RENDIMIENTO_GLOBAL', 'ID']))\n",
        "X_test_final = procesar_datos(df_test.drop(columns=['ID']))\n",
        "y = df_train['RENDIMIENTO_GLOBAL']\n",
        "\n",
        "\n",
        "# CODIFICACIÓN FINAL\n",
        "\n",
        "# Codificar variables categóricas restantes (Departamento, Programa, etc.)\n",
        "#Se usa OrdinalEncoder para asegurar una compatibilidad simple.\n",
        "\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "X[cat_cols] = encoder.fit_transform(X[cat_cols])\n",
        "X_test_final[cat_cols] = encoder.transform(X_test_final[cat_cols])\n",
        "\n",
        "# Codificar el TARGET (y)\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y) # Convierte alto, bajo... en 0, 1, 2, 3\n",
        "\n",
        "\n",
        "#  MODELADO CON XGBOOST\n",
        "# División Train/Val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded)\n",
        "\n",
        "print(\"Entrenando XGBoost...\")\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective='multi:softprob',\n",
        "    num_class=4,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    tree_method='hist',\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=50\n",
        ")\n",
        "\n",
        "# 5. EVALUACIÓN\n",
        "\n",
        "print(\"\\nEvaluando modelo...\")\n",
        "y_pred_val = model.predict(X_val)\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred_val))\n",
        "print(classification_report(y_val, y_pred_val, target_names=le.classes_))\n",
        "\n",
        "\n",
        "# 6. CREACIÓN DE SUBMISSION\n",
        "print(\"Generando predicciones finales...\")\n",
        "preds_encoded = model.predict(X_test_final)\n",
        "preds_labels = le.inverse_transform(preds_encoded) # Volver a texto (alto, bajo...)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': df_test['ID'],\n",
        "    'RENDIMIENTO_GLOBAL': preds_labels\n",
        "})\n",
        "\n",
        "submission.to_csv('submission_xgb.csv', index=False)\n",
        "print(\"ARCHIVO 'submission_xgb.csv' CREADO CON ÉXITO.\")"
      ]
    }
  ]
}