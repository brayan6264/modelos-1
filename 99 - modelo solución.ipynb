{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**PROYECTO KAGGLE - Pruebas Saber Pro Colombia**\n",
        "\n",
        "Modelo Predictivo a partir de la tabla de datos train.csv sobre el rendimiento de los estudiantes en las pruebas Saber Pro.\n",
        "\n",
        "Hecho por:\n",
        "- Juan Manuel Areiza Ospina - C.C. 1018226898\n",
        "- Samuel Puerta Patiño - C.C. 1023624795\n",
        "- Brayan Stiven Gómez Villa - C.C 1018224235"
      ],
      "metadata": {
        "id": "gCIJO_awQ40B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAbEHy7HPsMa"
      },
      "source": [
        "\n",
        "\n",
        "Se implementa un modelo de clasificación para predecir el rendimiento global de estudiantes en las Pruebas Saber Pro de Colombia, utilizando XGBoost con optimización de hiperparámetros mediante Optuna y pseudo-etiquetado.\n",
        "\n",
        "El contexto del proyecto es predecir el desempeño de estudiantes (bajo, medio-bajo, medio-alto, alto) basado en datos socioeconómicos, institucionales y estadísticos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwAlff1KPsMb"
      },
      "source": [
        "## Importaciones\n",
        "\n",
        "En esta sección se importan las bibliotecas necesarias para el procesamiento de datos, modelado y optimización. Pandas y NumPy para manejo de datos, Optuna para optimización de hiperparámetros, XGBoost como modelo de clasificación, y herramientas de scikit-learn para validación cruzada y preprocesamiento."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy optuna xgboost scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhQaK4sWQGD4",
        "outputId": "1a7e7977-95cb-49eb-e68e-990d7d0341c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhIEC8MvPsMc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pandas.api.types import CategoricalDtype\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'\n",
        "!chmod 600 ./kaggle.json\n",
        "!kaggle competitions download -c udea-ai-4-eng-20252-pruebas-saber-pro-colombia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycc5ed4pR5jF",
        "outputId": "196f9adb-6bc5-49c7-b94d-19d9ba2752ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip to /content\n",
            "\r  0% 0.00/29.9M [00:00<?, ?B/s]\n",
            "\r100% 29.9M/29.9M [00:00<00:00, 848MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Duph5K9R59x",
        "outputId": "12957650-42d5-4825-cc67-17edf53104db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  udea-ai-4-eng-20252-pruebas-saber-pro-colombia.zip\n",
            "  inflating: submission_example.csv  \n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv                \n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0q7Voh0PsMc"
      },
      "source": [
        "## Configuración\n",
        "\n",
        "Aquí se definen las constantes y configuraciones del script, como los nombres de archivos, la columna objetivo, el estado aleatorio para reproducibilidad, el número de pruebas de Optuna y el umbral para pseudo-etiquetado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f10eZP9QPsMd"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "TRAIN_FILE = \"train.csv\"\n",
        "TEST_FILE = \"test.csv\"\n",
        "SUBMISSION_FILE = \"submission_advanced_v2.csv\"\n",
        "TARGET_COL = \"RENDIMIENTO_GLOBAL\"\n",
        "RANDOM_STATE = 42\n",
        "N_TRIALS = 10  # Increased to 20\n",
        "PSEUDO_LABEL_THRESHOLD = 0.90 # Back to 0.90 for safety"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yGtmUxGPsMd"
      },
      "source": [
        "## Funciones Auxiliares\n",
        "\n",
        "Estas funciones ayudan en la carga de datos, limpieza de texto y ingeniería de características. Son cruciales para preparar los datos antes del modelado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SusPpyxbPsMd"
      },
      "source": [
        "### Función load_data\n",
        "\n",
        "Esta función carga los datos de entrenamiento y prueba desde los archivos CSV especificados. Verifica que el archivo de entrenamiento exista y devuelve los DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a2hnFbfPsMd"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    print(\"Loading data...\")\n",
        "    if not os.path.exists(TRAIN_FILE):\n",
        "        raise FileNotFoundError(f\"Train file not found at {os.path.abspath(TRAIN_FILE)}\")\n",
        "\n",
        "    train_df = pd.read_csv(TRAIN_FILE)\n",
        "    test_df = pd.read_csv(TEST_FILE)\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9RwkhJZPsMe"
      },
      "source": [
        "### Función clean_text\n",
        "\n",
        "Esta función limpia y estandariza las columnas de texto en el DataFrame. Elimina espacios en blanco, estandariza respuestas Sí/No y corrige inconsistencias en el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRJqe42OPsMe"
      },
      "outputs": [],
      "source": [
        "def clean_text(df):\n",
        "    # Standardize text columns\n",
        "    df.columns = df.columns.str.strip()\n",
        "    # Map all string columns to strip whitespace\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "    # Standardize Yes/No\n",
        "    replace_dict = {\n",
        "        \"Si\": \"Sí\", \"si\": \"Sí\", \"sÃ­\": \"Sí\",\n",
        "        \"N\": \"No\", \"NO\": \"No\", \"n\": \"No\"\n",
        "    }\n",
        "    df.replace(replace_dict, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnAOZagPsMe"
      },
      "source": [
        "### Función feature_engineering\n",
        "\n",
        "Esta función realiza la ingeniería de características: elimina la columna ID, maneja la alta cardinalidad en programas académicos, codifica ordinalmente variables categóricas, crea índices socioeconómicos y de educación parental, y calcula presión financiera. También elimina columnas ruidosas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSgEqPs_PsMe",
        "outputId": "702b86d9-3a76-474c-cb97-6ac19a7e7114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:44: SyntaxWarning: invalid escape sequence '\\d'\n",
            "<>:44: SyntaxWarning: invalid escape sequence '\\d'\n",
            "/tmp/ipython-input-3484362777.py:44: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  df[\"F_ESTRATOVIVIENDA_NUM\"] = df[\"F_ESTRATOVIVIENDA\"].astype(str).str.extract('(\\d+)').astype(float).fillna(0)\n"
          ]
        }
      ],
      "source": [
        "def feature_engineering(df, is_train=True, allowed_programs=None):\n",
        "    print(\"Engineering features...\")\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Drop ID\n",
        "    if 'ID' in df.columns:\n",
        "        df = df.drop(columns=['ID'])\n",
        "\n",
        "    # 2. Handle E_PRGM_ACADEMICO (High Cardinality)\n",
        "    if 'E_PRGM_ACADEMICO' in df.columns:\n",
        "        if is_train:\n",
        "            frecuencias = df[\"E_PRGM_ACADEMICO\"].value_counts()\n",
        "            allowed_programs = set(frecuencias.head(200).index) # Increased to 250 to match best model\n",
        "            df[\"E_PRGM_ACADEMICO\"] = df[\"E_PRGM_ACADEMICO\"].apply(lambda x: x if x in allowed_programs else \"OTROS_PROGRAMAS\")\n",
        "        else:\n",
        "            if allowed_programs is not None:\n",
        "                df[\"E_PRGM_ACADEMICO\"] = df[\"E_PRGM_ACADEMICO\"].apply(lambda x: x if x in allowed_programs else \"OTROS_PROGRAMAS\")\n",
        "\n",
        "    # 3. Ordinal Encoding & Numeric Conversion\n",
        "\n",
        "    # Hours Worked\n",
        "    orden_horas = ['0', \"Menos de 10 horas\", \"Entre 11 y 20 horas\", \"Entre 21 y 30 horas\", \"Más de 30 horas\"]\n",
        "    horas_map = {k: i for i, k in enumerate(orden_horas)}\n",
        "    if \"E_HORASSEMANATRABAJA\" in df.columns:\n",
        "        df[\"E_HORASSEMANATRABAJA_NUM\"] = df[\"E_HORASSEMANATRABAJA\"].map(horas_map).fillna(0)\n",
        "\n",
        "    # Education Level\n",
        "    orden_educacion = [\n",
        "        \"Ninguno\", \"No sabe\", \"No Aplica\", \"Primaria incompleta\", \"Primaria completa\",\n",
        "        \"Secundaria (Bachillerato) incompleta\", \"Secundaria (Bachillerato) completa\",\n",
        "        \"Técnica o tecnológica incompleta\", \"Técnica o tecnológica completa\",\n",
        "        \"Educación profesional incompleta\", \"Educación profesional completa\", \"Postgrado\"\n",
        "    ]\n",
        "    # Custom mapping to give \"Postgrado\" higher value\n",
        "    edu_map = {k: i for i, k in enumerate(orden_educacion)}\n",
        "\n",
        "    for col in [\"F_EDUCACIONPADRE\", \"F_EDUCACIONMADRE\"]:\n",
        "        if col in df.columns:\n",
        "            df[f\"{col}_NUM\"] = df[col].map(edu_map).fillna(-1)\n",
        "\n",
        "    # Estrato\n",
        "    if \"F_ESTRATOVIVIENDA\" in df.columns:\n",
        "        # Extract number from \"Estrato 1\", etc.\n",
        "        df[\"F_ESTRATOVIVIENDA_NUM\"] = df[\"F_ESTRATOVIVIENDA\"].astype(str).str.extract('(\\d+)').astype(float).fillna(0)\n",
        "\n",
        "    # 4. New Interaction Features\n",
        "\n",
        "    # Socioeconomic Index\n",
        "    # Map Yes/No to 1/0\n",
        "    yes_no_cols = ['F_TIENEINTERNET', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', 'F_TIENECOMPUTADOR']\n",
        "    for col in yes_no_cols:\n",
        "        if col in df.columns:\n",
        "            df[f\"{col}_NUM\"] = df[col].apply(lambda x: 1 if x == 'Sí' else 0)\n",
        "\n",
        "    # Combine duplicates if F_TIENEINTERNET.1 exists\n",
        "    if 'F_TIENEINTERNET.1' in df.columns:\n",
        "        df['F_TIENEINTERNET_COMBINED'] = df.apply(lambda row: 1 if (row.get('F_TIENEINTERNET') == 'Sí' or row.get('F_TIENEINTERNET.1') == 'Sí') else 0, axis=1)\n",
        "    elif 'F_TIENEINTERNET' in df.columns:\n",
        "        df['F_TIENEINTERNET_COMBINED'] = df['F_TIENEINTERNET_NUM']\n",
        "    else:\n",
        "        df['F_TIENEINTERNET_COMBINED'] = 0\n",
        "\n",
        "    # Create Index\n",
        "    df['SOCIOECONOMIC_INDEX'] = (\n",
        "        df.get('F_ESTRATOVIVIENDA_NUM', 0) +\n",
        "        df.get('F_TIENELAVADORA_NUM', 0) +\n",
        "        df.get('F_TIENEAUTOMOVIL_NUM', 0) +\n",
        "        df.get('F_TIENECOMPUTADOR_NUM', 0) +\n",
        "        df.get('F_TIENEINTERNET_COMBINED', 0)\n",
        "    )\n",
        "\n",
        "    # Parental Education Index\n",
        "    df['PARENT_EDU_INDEX'] = (df.get('F_EDUCACIONPADRE_NUM', 0) + df.get('F_EDUCACIONMADRE_NUM', 0)) / 2\n",
        "\n",
        "    # Financial Pressure Proxy (Tuition * Work Hours)\n",
        "    # Matricula is usually categorical ranges, let's map roughly\n",
        "    matricula_map = {\n",
        "        \"No pagó matrícula\": 0,\n",
        "        \"Menos de 500 mil\": 250000,\n",
        "        \"Entre 500 mil y menos de 1 millón\": 750000,\n",
        "        \"Entre 1 millón y menos de 2.5 millones\": 1750000,\n",
        "        \"Entre 2.5 millones y menos de 4 millones\": 3250000,\n",
        "        \"Entre 4 millones y menos de 5.5 millones\": 4750000,\n",
        "        \"Entre 5.5 millones y menos de 7 millones\": 6250000,\n",
        "        \"Más de 7 millones\": 8000000\n",
        "    }\n",
        "    if \"E_VALORMATRICULAUNIVERSIDAD\" in df.columns:\n",
        "        df[\"VALOR_MATRICULA_EST\"] = df[\"E_VALORMATRICULAUNIVERSIDAD\"].map(matricula_map).fillna(0)\n",
        "        df[\"FINANCIAL_PRESSURE\"] = df[\"VALOR_MATRICULA_EST\"] * df.get(\"E_HORASSEMANATRABAJA_NUM\", 0)\n",
        "\n",
        "    # 5. DROP NOISY COLUMNS (Crucial Step from Best Model)\n",
        "    # We used these to create indices, now we drop the raw ones that were deemed noisy\n",
        "    cols_to_drop = [\n",
        "        'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL',\n",
        "        'E_PRIVADO_LIBERTAD', 'F_TIENEINTERNET.1',\n",
        "        'INDICADOR_1', 'INDICADOR_2', 'INDICADOR_3', 'INDICADOR_4',\n",
        "        # Also drop the intermediate numeric cols we created if they are redundant,\n",
        "        # but let's keep the _NUM ones as they are better than raw text.\n",
        "        # We drop the original text ones if we have _NUM\n",
        "    ]\n",
        "    # Actually, let's strictly follow the best model's drop list + our used raw inputs\n",
        "    # Best model dropped: 'E_VALORMATRICULAUNIVERSIDAD', 'F_TIENELAVADORA', 'F_TIENEAUTOMOVIL', 'E_PRIVADO_LIBERTAD', 'E_PAGOMATRICULAPROPIO', 'F_TIENEINTERNET.1', 'INDICADOR_1'...'INDICADOR_4'\n",
        "\n",
        "    final_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "    df = df.drop(columns=final_drop)\n",
        "\n",
        "    return df, allowed_programs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukm02r6iPsMe"
      },
      "source": [
        "## Función Principal (main)\n",
        "\n",
        "Esta es la función principal que ejecuta todo el flujo: carga los datos, los limpia, realiza ingeniería de características, prepara los datos para el modelo, optimiza hiperparámetros con Optuna, aplica pseudo-etiquetado, hace predicciones finales y guarda los resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibXA8KX5PsMe"
      },
      "source": [
        "### Optimización con Optuna\n",
        "\n",
        "Se define una función objetivo para Optuna que prueba diferentes hiperparámetros del modelo XGBoost usando validación cruzada estratificada. El objetivo es maximizar la precisión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg0KgRdbPsMf"
      },
      "source": [
        "### Pseudo-Etiquetado\n",
        "\n",
        "Se entrena un modelo inicial con los mejores parámetros, se predicen probabilidades en el conjunto de prueba, y se seleccionan muestras de alta confianza para agregar al conjunto de entrenamiento, mejorando el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3uNWtP4PsMf"
      },
      "source": [
        "### Predicción Final y Guardado\n",
        "\n",
        "Se generan las predicciones finales en el conjunto de prueba, se decodifican las etiquetas, se crea el archivo de envío y se muestra la importancia de las características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DslfosNpPsMf",
        "outputId": "df677a10-dd6d-4aeb-9b66-a4d59a9133f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Engineering features...\n",
            "Engineering features...\n",
            "Categorical columns: ['E_PRGM_ACADEMICO', 'E_PRGM_DEPARTAMENTO', 'E_VALORMATRICULAUNIVERSIDAD', 'E_HORASSEMANATRABAJA', 'F_ESTRATOVIVIENDA', 'F_TIENEINTERNET', 'F_EDUCACIONPADRE', 'E_PAGOMATRICULAPROPIO', 'F_TIENECOMPUTADOR', 'F_EDUCACIONMADRE']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-28 02:28:30,169] A new study created in memory with name: no-name-03437c30-b4cb-4db8-bbcb-78e9be4918d9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Optuna with 10 trials...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-28 02:39:49,723] Trial 0 finished with value: 0.4382324891915759 and parameters: {'learning_rate': 0.09674730429833721, 'max_depth': 4, 'subsample': 0.9406613372142963, 'colsample_bytree': 0.913100166784449, 'reg_alpha': 4.509134522158807, 'reg_lambda': 1.6290289661682364, 'n_estimators': 527, 'min_child_weight': 5}. Best is trial 0 with value: 0.4382324891915759.\n",
            "[I 2025-11-28 02:53:19,232] Trial 1 finished with value: 0.43474368049393736 and parameters: {'learning_rate': 0.15541921878820836, 'max_depth': 7, 'subsample': 0.9204061751765162, 'colsample_bytree': 0.9602712117803982, 'reg_alpha': 4.517519428929507, 'reg_lambda': 2.3200177978757943, 'n_estimators': 359, 'min_child_weight': 3}. Best is trial 0 with value: 0.4382324891915759.\n",
            "[I 2025-11-28 03:01:03,663] Trial 2 finished with value: 0.43803321120698 and parameters: {'learning_rate': 0.13099655241901237, 'max_depth': 5, 'subsample': 0.8625472807214325, 'colsample_bytree': 0.8228280294857987, 'reg_alpha': 1.3425539527529795, 'reg_lambda': 0.14522117106544702, 'n_estimators': 289, 'min_child_weight': 1}. Best is trial 0 with value: 0.4382324891915759.\n",
            "[I 2025-11-28 03:06:44,413] Trial 3 finished with value: 0.43675234512713473 and parameters: {'learning_rate': 0.1980439534109123, 'max_depth': 4, 'subsample': 0.7300693117357844, 'colsample_bytree': 0.7767144229495481, 'reg_alpha': 0.11270685149004994, 'reg_lambda': 3.5062946005251554, 'n_estimators': 253, 'min_child_weight': 3}. Best is trial 0 with value: 0.4382324891915759.\n",
            "[I 2025-11-28 03:17:28,192] Trial 4 finished with value: 0.43473646031695073 and parameters: {'learning_rate': 0.03876214025305404, 'max_depth': 4, 'subsample': 0.7505688174232494, 'colsample_bytree': 0.9193273943629063, 'reg_alpha': 4.057879437800398, 'reg_lambda': 1.3027568275824928, 'n_estimators': 480, 'min_child_weight': 5}. Best is trial 0 with value: 0.4382324891915759.\n",
            "[W 2025-11-28 03:42:15,035] Trial 5 failed with parameters: {'learning_rate': 0.08313600294229437, 'max_depth': 9, 'subsample': 0.7298346429314301, 'colsample_bytree': 0.6461610029478149, 'reg_alpha': 0.1664064376023752, 'reg_lambda': 2.253396757812817, 'n_estimators': 464, 'min_child_weight': 5} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2955533712.py\", line 58, in objective\n",
            "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 684, in cross_val_score\n",
            "    cv_results = cross_validate(\n",
            "                 ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 411, in cross_validate\n",
            "    results = parallel(\n",
            "              ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 1986, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    test_scores = _score(\n",
            "                  ^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 949, in _score\n",
            "    scores = scorer(estimator, X_test, y_test, **score_params)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 140, in __call__\n",
            "    score = scorer._score(\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 380, in _score\n",
            "    y_pred = method_caller(\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
            "    result, _ = _get_response_values(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_response.py\", line 214, in _get_response_values\n",
            "    y_pred = prediction_method(X)\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/xgboost/core.py\", line 774, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\", line 1842, in predict\n",
            "    class_probs = super().predict(\n",
            "                  ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/xgboost/core.py\", line 774, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\", line 1446, in predict\n",
            "    predts = self.get_booster().inplace_predict(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/xgboost/core.py\", line 774, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/xgboost/core.py\", line 2888, in inplace_predict\n",
            "    _LIB.XGBoosterPredictFromColumnar(\n",
            "KeyboardInterrupt\n",
            "[W 2025-11-28 03:42:15,045] Trial 5 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2955533712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2955533712.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Optuna with {N_TRIALS} trials...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_TRIALS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best CV Accuracy: {study.best_value:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     ):\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2955533712.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_STATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# independent, and that it is pickle-able.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     results = parallel(\n\u001b[0m\u001b[1;32m    412\u001b[0m         delayed(_fit_and_score)(\n\u001b[1;32m    413\u001b[0m             \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         test_scores = _score(\n\u001b[0m\u001b[1;32m    889\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_params_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, score_params, error_score)\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mscore_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MultimetricScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     score = scorer._score(\n\u001b[0m\u001b[1;32m    141\u001b[0m                         \u001b[0mcached_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pos_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mresponse_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_response_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         y_pred = method_caller(\n\u001b[0m\u001b[1;32m    381\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0m_get_response_method_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, response_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse_method\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     result, _ = _get_response_values(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_response.py\u001b[0m in \u001b[0;36m_get_response_values\u001b[0;34m(estimator, X, response_method, pos_label, return_response_method_used)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mpos_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprediction_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"predict_proba\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_log_proba\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1840\u001b[0m     ) -> ArrayLike:\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1843\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m                 \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1447\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mArrowTransformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPandasTransformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m             _check_call(\n\u001b[0;32m-> 2888\u001b[0;31m                 _LIB.XGBoosterPredictFromColumnar(\n\u001b[0m\u001b[1;32m   2889\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # 1. Load\n",
        "    train_df_raw, test_df_raw = load_data()\n",
        "    test_ids = test_df_raw['ID']\n",
        "\n",
        "    # 2. Clean & Preprocess\n",
        "    train_df_clean = clean_text(train_df_raw)\n",
        "    test_df_clean = clean_text(test_df_raw)\n",
        "\n",
        "    train_df, allowed_programs = feature_engineering(train_df_clean, is_train=True)\n",
        "    test_df, _ = feature_engineering(test_df_clean, is_train=False, allowed_programs=allowed_programs)\n",
        "\n",
        "        # Prepare X, y\n",
        "    X = train_df.drop(columns=[TARGET_COL], errors='ignore')\n",
        "    y_raw = train_df[TARGET_COL]\n",
        "    X_test = test_df.drop(columns=['ID'], errors='ignore')\n",
        "\n",
        "    # Align columns\n",
        "    missing_cols = set(X.columns) - set(X_test.columns)\n",
        "    for c in missing_cols:\n",
        "        X_test[c] = 0\n",
        "    X_test = X_test[X.columns]\n",
        "\n",
        "    # Encode Target\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "\n",
        "    # Convert object columns to category for XGBoost\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    print(f\"Categorical columns: {categorical_cols}\")\n",
        "    for col in categorical_cols:\n",
        "        X[col] = X[col].astype(\"category\")\n",
        "        X_test[col] = X_test[col].astype(\"category\")\n",
        "\n",
        "    # 3. Optuna Optimization\n",
        "    def objective(trial):\n",
        "        param = {\n",
        "            'objective': 'multi:softmax',\n",
        "            'num_class': len(le.classes_),\n",
        "            'tree_method': 'hist',\n",
        "            'enable_categorical': True,\n",
        "            'eval_metric': 'mlogloss',\n",
        "            'random_state': RANDOM_STATE,\n",
        "            'n_jobs': -1,\n",
        "\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
        "            'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 5),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 5),\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 5)\n",
        "        }\n",
        "\n",
        "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
        "        model = xgb.XGBClassifier(**param)\n",
        "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
        "        return scores.mean()\n",
        "\n",
        "    print(f\"Starting Optuna with {N_TRIALS} trials...\")\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    print(f\"Best CV Accuracy: {study.best_value:.4f}\")\n",
        "    best_params = study.best_params\n",
        "\n",
        "    # 4. Pseudo-Labeling\n",
        "    print(\"\\n--- Pseudo-Labeling Phase ---\")\n",
        "\n",
        "    # Train initial best model\n",
        "    model_params = best_params.copy()\n",
        "    model_params.update({\n",
        "        'objective': 'multi:softmax',\n",
        "        'num_class': len(le.classes_),\n",
        "        'tree_method': 'hist',\n",
        "        'enable_categorical': True,\n",
        "        'random_state': RANDOM_STATE,\n",
        "        'n_jobs': -1\n",
        "    })\n",
        "\n",
        "    initial_model = xgb.XGBClassifier(**model_params)\n",
        "    initial_model.fit(X, y)\n",
        "\n",
        "    # Predict probabilities on test\n",
        "    probs = initial_model.predict_proba(X_test)\n",
        "    max_probs = np.max(probs, axis=1)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    # Select high confidence samples\n",
        "    high_conf_indices = np.where(max_probs > PSEUDO_LABEL_THRESHOLD)[0]\n",
        "    print(f\"Found {len(high_conf_indices)} pseudo-labels with confidence > {PSEUDO_LABEL_THRESHOLD}\")\n",
        "\n",
        "    if len(high_conf_indices) > 0:\n",
        "        X_pseudo = X_test.iloc[high_conf_indices]\n",
        "        y_pseudo = preds[high_conf_indices]\n",
        "\n",
        "        # Combine with original train\n",
        "        X_augmented = pd.concat([X, X_pseudo], axis=0)\n",
        "        y_augmented = np.concatenate([y, y_pseudo], axis=0)\n",
        "\n",
        "        print(f\"Retraining with augmented size: {len(X_augmented)}\")\n",
        "        final_model = xgb.XGBClassifier(**model_params)\n",
        "        final_model.fit(X_augmented, y_augmented)\n",
        "    else:\n",
        "        print(\"No pseudo-labels added. Using initial model.\")\n",
        "        final_model = initial_model\n",
        "\n",
        "        # 5. Final Prediction\n",
        "    print(\"Generating final predictions...\")\n",
        "    final_preds = final_model.predict(X_test)\n",
        "    final_preds_decoded = le.inverse_transform(final_preds)\n",
        "\n",
        "    submission = pd.DataFrame({'ID': test_ids, 'RENDIMIENTO_GLOBAL': final_preds_decoded})\n",
        "    submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "    print(f\"Saved {SUBMISSION_FILE}\")\n",
        "\n",
        "    # Feature Importance\n",
        "    importance = final_model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "    fi_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "    print(fi_df.sort_values(by='Importance', ascending=False).head(20))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}